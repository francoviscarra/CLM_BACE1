{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 20:10:40.305620: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-19 20:10:40.306018: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-19 20:10:40.308074: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-19 20:10:40.313745: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-19 20:10:40.323421: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-19 20:10:40.326169: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-19 20:10:40.333955: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-19 20:10:41.314637: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, LSTM, TimeDistributed, BatchNormalization, Dropout, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import pandas as pd \n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLM():\n",
    "    def __init__(self, layers, n_chars, dropouts, trainables,lr):\n",
    "        self.layers = layers\n",
    "        self.n_chars = n_chars\n",
    "        self.dropouts = dropouts\n",
    "        self.trainables = trainables\n",
    "        self.lr = lr\n",
    "        self.model = None\n",
    "        self.build_model()\n",
    "    def build_model(self):\n",
    "        # create a squential model and assigned to self.model\n",
    "        self.model = Sequential()\n",
    "        # Add a BatchNormalization layer input layer, input shape should be None, number of characters\n",
    "        self.model.add(BatchNormalization(input_shape = (None,self.n_chars)))\n",
    "        \n",
    "        # Add n LSTM layers, where n is defined by the variable layers. This will be a list of ints where each int is the number of units in the LSTM layer. i.e [256, 512] will add two layers with 256 and 512 neurons respectively\n",
    "        # This loop should also include the dropout values contained in the dropouts variable (list of floats). \n",
    "        # And it should also include the trainable parameter from the trainables variable (list of booleans). \n",
    "        print(self.layers)\n",
    "        for i in range(len(self.layers)):\n",
    "            if i ==len(self.layers)-1:\n",
    "                print(self.layers[i], \"False\")\n",
    "                self.model.add(LSTM(units=self.layers[i], trainable=self.trainables[i], return_sequences=False, dropout=self.dropouts[i]))\n",
    "            else:\n",
    "                print(self.layers[i], \"True\")\n",
    "                self.model.add(LSTM(units=self.layers[i], trainable=self.trainables[i], return_sequences=True, dropout=self.dropouts[i]))\n",
    "            \n",
    "        # add another BatchNormalization layer, this time no need to specify input shape\n",
    "        self.model.add(BatchNormalization())\n",
    "        # Finally add an output layer, it should be a Dense layer with n_chars units and softmax activation. Also, this layer should be containd in a TimeDistributed layer, so that it can be applied to each character in the sequence.\n",
    "\n",
    "        # compile the model with Adam optimizer and categorical_crossentropy loss. Don't forget to set the learning rate to the value contained in the lr variable with optimizer = Adam(learning_rate=self.lr)\n",
    "        optimizer = Adam(learning_rate=self.lr)\n",
    "        self.model.compile(optimizer=optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "#Fixed parameters\n",
    "PROCESSING_FIXED = {'start_char': 'G', \n",
    "                    'end_char': 'E', \n",
    "                    'pad_char': 'A'}\n",
    "\n",
    "INDICES_TOKEN = {0: 'c',\n",
    "                 1: 'C',\n",
    "                 2: '(',\n",
    "                 3: ')',\n",
    "                 4: 'O',\n",
    "                 5: '1',\n",
    "                 6: '2',\n",
    "                 7: '=',\n",
    "                 8: 'N',\n",
    "                 9: '@',\n",
    "                 10: '[',\n",
    "                 11: ']',\n",
    "                 12: 'n',\n",
    "                 13: '3',\n",
    "                 14: 'H',\n",
    "                 15: 'F',\n",
    "                 16: '4',\n",
    "                 17: '-',\n",
    "                 18: 'S',\n",
    "                 19: 'Cl',\n",
    "                 20: '/',\n",
    "                 21: 's',\n",
    "                 22: 'o',\n",
    "                 23: '5',\n",
    "                 24: '+',\n",
    "                 25: '#',\n",
    "                 26: '\\\\',\n",
    "                 27: 'Br',\n",
    "                 28: 'P',\n",
    "                 29: '6',\n",
    "                 30: 'I',\n",
    "                 31: '7',\n",
    "                 32: PROCESSING_FIXED['start_char'],\n",
    "                 33: PROCESSING_FIXED['end_char'],\n",
    "                 34: PROCESSING_FIXED['pad_char']}                \n",
    "TOKEN_INDICES = {v: k for k, v in INDICES_TOKEN.items()}\n",
    "\n",
    "def smi_tokenizer(smi):\n",
    "    \"\"\"\n",
    "    Tokenize a SMILES\n",
    "    \"\"\"\n",
    "    pattern =  \"(\\[|\\]|Xe|Ba|Rb|Ra|Sr|Dy|Li|Kr|Bi|Mn|He|Am|Pu|Cm|Pm|Ne|Th|Ni|Pr|Fe|Lu|Pa|Fm|Tm|Tb|Er|Be|Al|Gd|Eu|te|As|Pt|Lr|Sm|Ca|La|Ti|Te|Ac|Si|Cf|Rf|Na|Cu|Au|Nd|Ag|Se|se|Zn|Mg|Br|Cl|U|V|K|C|B|H|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%\\d{2}|\\d)\"\n",
    "    regex = re.compile(pattern)\n",
    "    tokens = [token for token in regex.findall(smi)]\n",
    "\n",
    "    return tokens        \n",
    "\n",
    "class onehotencoder():\n",
    "    def __init__(self, max_chars=90):\n",
    "        # here we define the class variables that will be used in the functions. If we want to use them in the class, we need to use self. in front of the variable name\n",
    "        # then when we want to use the class variable in the function, we pass self as the first argument to the function and we can access the class variables\n",
    "        self.max_chars = max_chars\n",
    "    def smiles_to_num(self, smiles):\n",
    "        tokens = smi_tokenizer(smiles)\n",
    "        tokens = [PROCESSING_FIXED['start_char']] + tokens +[PROCESSING_FIXED['end_char']]\n",
    "        tokens+= [PROCESSING_FIXED['pad_char']]* (self.max_chars-len(tokens))\n",
    "        num_tokens = [TOKEN_INDICES[t] for t in tokens]\n",
    "        #print(tokens)\n",
    "        #create numbers\n",
    "        return np.asarray(tokens) #Return the numbers as a numpy array\n",
    "    def num_to_onehot(self,tokens):\n",
    "        onehot = np.zeros((len(tokens), len(INDICES_TOKEN)))\n",
    "        for i,token in enumerate(tokens):\n",
    "            onehot[i, TOKEN_INDICES[token]] = 1\n",
    "        return onehot\n",
    "        #This function will convert the numbers to one hot encoding. It should take a list of numbers and return a list of one hot encoded vectors with the shape (max_chars, 35)\n",
    "    def generate_data(self, smiles):\n",
    "        nums = self.smiles_to_num(smiles)\n",
    "        data = self.num_to_onehot(nums)\n",
    "        return np.asarray(data)\n",
    "        #This function should take a list of smiles strings and return a numpy array of one hot encoded vectors with the shape (number of smiles, max_chars, 35)\n",
    "        #You can use the smiles_to_num and num_to_onehot functions to help you with this\n",
    "        #This function will be used to generate the training and validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1726791042.860702   12632 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-19 20:10:42.861930: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "chem_model = load_model(f'../pretraining/LSTM/{48:03d}.keras') #load the model from file\n",
    "print('model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12632/3666820747.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  beta[\"activity_class\"] = beta[\"activity_class\"].replace(\"very_active\", 2)\n"
     ]
    }
   ],
   "source": [
    "beta = pd.read_csv('beta_activity_class.csv') #Clean CSV file with beta secretase smiles and activity\n",
    "beta[\"activity_class\"].value_counts()\n",
    "#dropna of activity_class\n",
    "beta = beta.dropna(subset=[\"activity_class\"])\n",
    "#transfor activity_class to 0,1,2\n",
    "beta[\"activity_class\"] = beta[\"activity_class\"].replace(\"moderately_active\", 1)\n",
    "beta[\"activity_class\"] = beta[\"activity_class\"].replace(\"inactive\", 0)\n",
    "beta[\"activity_class\"] = beta[\"activity_class\"].replace(\"very_active\", 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ligand SMILES</th>\n",
       "      <th>activity_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCSc1cnc(cn1)C(=O)Nc1cccc(c1)[C@]1(C)CCSC(N)=N1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C[C@]1(CCSC(N)=N1)c1cc(NC(=O)CCc2ccc(O)c(O)c2)...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COc1nc(nc(C)c1F)N1C[C@H]2C(=O)N(C)C(=N)N[C@]2(...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COc1nc(nc(C)c1F)N1C[C@H]2C(=O)N(C)C(=N)N[C@]2(...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COc1nc(nc(C)c1F)N1C[C@H]2C(=O)N(C)C(=N)N[C@]2(...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15992</th>\n",
       "      <td>FC(F)(F)c1ccc(N\\N=C\\c2coc3cc4oc(cc4cc3c2=O)-c2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15993</th>\n",
       "      <td>COc1ccc(cc1)-c1cc2cc3c(cc2o1)occ(\\C=N\\Nc1ccc(c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15994</th>\n",
       "      <td>COc1ccc(CN(CCN(C)CCN)c2ccccn2)cc1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>Fc1cccc(c1)-c1cc2c(ccc3c2occ(\\C=N\\Nc2ccc(cc2)C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>CCC(=O)CNC(=O)Nc1nc2ccc(cc2s1)-c1oc2c(OC)c(O)c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13155 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Ligand SMILES  activity_class\n",
       "0        CCSc1cnc(cn1)C(=O)Nc1cccc(c1)[C@]1(C)CCSC(N)=N1               1\n",
       "1      C[C@]1(CCSC(N)=N1)c1cc(NC(=O)CCc2ccc(O)c(O)c2)...               1\n",
       "2      COc1nc(nc(C)c1F)N1C[C@H]2C(=O)N(C)C(=N)N[C@]2(...               1\n",
       "3      COc1nc(nc(C)c1F)N1C[C@H]2C(=O)N(C)C(=N)N[C@]2(...               1\n",
       "4      COc1nc(nc(C)c1F)N1C[C@H]2C(=O)N(C)C(=N)N[C@]2(...               1\n",
       "...                                                  ...             ...\n",
       "15992  FC(F)(F)c1ccc(N\\N=C\\c2coc3cc4oc(cc4cc3c2=O)-c2...               0\n",
       "15993  COc1ccc(cc1)-c1cc2cc3c(cc2o1)occ(\\C=N\\Nc1ccc(c...               0\n",
       "15994                  COc1ccc(CN(CCN(C)CCN)c2ccccn2)cc1               0\n",
       "15995  Fc1cccc(c1)-c1cc2c(ccc3c2occ(\\C=N\\Nc2ccc(cc2)C...               0\n",
       "15996  CCC(=O)CNC(=O)Nc1nc2ccc(cc2s1)-c1oc2c(OC)c(O)c...               1\n",
       "\n",
       "[13155 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also, remove any smiles string that contains a character NOT in our vocabulary (excluding pad, start and end chars). Hint: allowed_chars = [t for t in TOKEN_INDICES.keys()][:-3]\n",
    "allowed_chars = [t for t in TOKEN_INDICES.keys()][:-3]\n",
    "beta = beta[beta['Ligand SMILES'].apply(lambda x: all(char in allowed_chars for char in x))]\n",
    "#drop data longer than 90 characters\n",
    "beta = beta[beta['Ligand SMILES'].apply(lambda x: len(x)<=90)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ligand SMILES</th>\n",
       "      <th>activity_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCSc1cnc(cn1)C(=O)Nc1cccc(c1)[C@]1(C)CCSC(N)=N1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C[C@]1(CCSC(N)=N1)c1cc(NC(=O)CCc2ccc(O)c(O)c2)...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COc1nc(nc(C)c1F)N1C[C@H]2C(=O)N(C)C(=N)N[C@]2(...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nc1ccc2ccccc2n1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nc1cccc(CCc2ccccc2)n1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15991</th>\n",
       "      <td>Fc1cccc(c1)-c1cc2cc3c(cc2o1)occ(\\C=N\\Nc1ccc(cc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15993</th>\n",
       "      <td>COc1ccc(cc1)-c1cc2cc3c(cc2o1)occ(\\C=N\\Nc1ccc(c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15994</th>\n",
       "      <td>COc1ccc(CN(CCN(C)CCN)c2ccccn2)cc1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>Fc1cccc(c1)-c1cc2c(ccc3c2occ(\\C=N\\Nc2ccc(cc2)C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>CCC(=O)CNC(=O)Nc1nc2ccc(cc2s1)-c1oc2c(OC)c(O)c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9300 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Ligand SMILES  activity_class\n",
       "0        CCSc1cnc(cn1)C(=O)Nc1cccc(c1)[C@]1(C)CCSC(N)=N1               1\n",
       "1      C[C@]1(CCSC(N)=N1)c1cc(NC(=O)CCc2ccc(O)c(O)c2)...               1\n",
       "2      COc1nc(nc(C)c1F)N1C[C@H]2C(=O)N(C)C(=N)N[C@]2(...               1\n",
       "5                                        Nc1ccc2ccccc2n1               0\n",
       "6                                  Nc1cccc(CCc2ccccc2)n1               0\n",
       "...                                                  ...             ...\n",
       "15991  Fc1cccc(c1)-c1cc2cc3c(cc2o1)occ(\\C=N\\Nc1ccc(cc...               0\n",
       "15993  COc1ccc(cc1)-c1cc2cc3c(cc2o1)occ(\\C=N\\Nc1ccc(c...               0\n",
       "15994                  COc1ccc(CN(CCN(C)CCN)c2ccccn2)cc1               0\n",
       "15995  Fc1cccc(c1)-c1cc2c(ccc3c2occ(\\C=N\\Nc2ccc(cc2)C...               0\n",
       "15996  CCC(=O)CNC(=O)Nc1nc2ccc(cc2s1)-c1oc2c(OC)c(O)c...               1\n",
       "\n",
       "[9300 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list with the one hot vectors of the smiles\n",
    "encoder = onehotencoder(max_chars=92)\n",
    "smiles_arr = [encoder.generate_data(s) for s in beta['Ligand SMILES']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_x = np.zeros((9300,92,35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9300):\n",
    "    inputs_x[i] = smiles_arr[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/annawis3/.local/lib/python3.10/site-packages/keras/src/layers/normalization/batch_normalization.py:143: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1024, 256]\n",
      "1024 True\n",
      "256 False\n",
      "model created\n"
     ]
    }
   ],
   "source": [
    "#compile the new model\n",
    "layers = [1024,256]\n",
    "dropouts = [0.40, 0.40]\n",
    "trainables = [True, True]\n",
    "lr = 0.001\n",
    "epochs = 80\n",
    "new_model = CLM(\n",
    "    layers=layers,\n",
    "    dropouts=dropouts,\n",
    "    trainables=trainables,\n",
    "    lr=lr,\n",
    "    n_chars=35\n",
    "    )\n",
    "print('model created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the weights of the new model to the weights of the old model except the last layer\n",
    "new_model.model.set_weights(chem_model.get_weights()[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 126ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9300, 92, 35)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict from smiles arr and get the last hidden state of the LSTM\n",
    "hidden = chem_model.predict(inputs_x)\n",
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = hidden.reshape(9300, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the features to a csv file\n",
    "features_df = pd.DataFrame(features)\n",
    "features_df.to_csv('features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/annawis3/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7089 - loss: 0.7682 - val_accuracy: 0.7532 - val_loss: 0.6433\n",
      "Epoch 2/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7402 - loss: 0.6227 - val_accuracy: 0.7731 - val_loss: 0.5894\n",
      "Epoch 3/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7759 - loss: 0.5364 - val_accuracy: 0.7704 - val_loss: 0.5736\n",
      "Epoch 4/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7989 - loss: 0.4818 - val_accuracy: 0.7796 - val_loss: 0.5572\n",
      "Epoch 5/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8246 - loss: 0.4357 - val_accuracy: 0.7860 - val_loss: 0.5780\n",
      "Epoch 6/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8316 - loss: 0.3979 - val_accuracy: 0.7844 - val_loss: 0.5793\n",
      "Epoch 7/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8475 - loss: 0.3645 - val_accuracy: 0.7941 - val_loss: 0.5697\n",
      "Epoch 8/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8590 - loss: 0.3438 - val_accuracy: 0.7909 - val_loss: 0.6015\n",
      "Epoch 9/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8744 - loss: 0.3041 - val_accuracy: 0.7935 - val_loss: 0.6231\n",
      "Epoch 10/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8773 - loss: 0.2926 - val_accuracy: 0.7962 - val_loss: 0.6776\n",
      "Epoch 11/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8784 - loss: 0.2838 - val_accuracy: 0.8027 - val_loss: 0.6699\n",
      "Epoch 12/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8875 - loss: 0.2675 - val_accuracy: 0.7984 - val_loss: 0.6509\n",
      "Epoch 13/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8896 - loss: 0.2684 - val_accuracy: 0.7952 - val_loss: 0.7034\n",
      "Epoch 14/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8989 - loss: 0.2399 - val_accuracy: 0.7957 - val_loss: 0.6598\n",
      "Epoch 15/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8972 - loss: 0.2357 - val_accuracy: 0.7828 - val_loss: 0.7100\n",
      "Epoch 16/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9015 - loss: 0.2288 - val_accuracy: 0.7812 - val_loss: 0.7389\n",
      "Epoch 17/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9005 - loss: 0.2381 - val_accuracy: 0.8000 - val_loss: 0.7446\n",
      "Epoch 18/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9031 - loss: 0.2172 - val_accuracy: 0.7935 - val_loss: 0.8382\n",
      "Epoch 19/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9080 - loss: 0.2153 - val_accuracy: 0.7839 - val_loss: 0.7749\n",
      "Epoch 20/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9083 - loss: 0.2014 - val_accuracy: 0.7876 - val_loss: 0.7553\n",
      "Epoch 21/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9086 - loss: 0.2059 - val_accuracy: 0.7962 - val_loss: 0.8373\n",
      "Epoch 22/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9155 - loss: 0.1969 - val_accuracy: 0.7941 - val_loss: 0.8123\n",
      "Epoch 23/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9112 - loss: 0.2006 - val_accuracy: 0.7898 - val_loss: 0.9204\n",
      "Epoch 24/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9198 - loss: 0.1885 - val_accuracy: 0.7957 - val_loss: 0.8792\n",
      "Epoch 25/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9167 - loss: 0.1909 - val_accuracy: 0.7925 - val_loss: 0.8613\n",
      "Epoch 26/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9111 - loss: 0.1920 - val_accuracy: 0.8048 - val_loss: 0.8884\n",
      "Epoch 27/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9173 - loss: 0.1783 - val_accuracy: 0.7946 - val_loss: 0.8104\n",
      "Epoch 28/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9171 - loss: 0.1896 - val_accuracy: 0.7887 - val_loss: 0.8761\n",
      "Epoch 29/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9162 - loss: 0.1792 - val_accuracy: 0.7946 - val_loss: 0.8780\n",
      "Epoch 30/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9216 - loss: 0.1708 - val_accuracy: 0.7968 - val_loss: 0.9603\n",
      "Epoch 31/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9219 - loss: 0.1655 - val_accuracy: 0.8005 - val_loss: 0.9425\n",
      "Epoch 32/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9167 - loss: 0.1725 - val_accuracy: 0.8086 - val_loss: 0.9885\n",
      "Epoch 33/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9210 - loss: 0.1716 - val_accuracy: 0.7855 - val_loss: 0.9983\n",
      "Epoch 34/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9146 - loss: 0.1823 - val_accuracy: 0.7978 - val_loss: 1.0171\n",
      "Epoch 35/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9179 - loss: 0.1664 - val_accuracy: 0.8022 - val_loss: 1.0464\n",
      "Epoch 36/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9261 - loss: 0.1640 - val_accuracy: 0.7968 - val_loss: 1.0127\n",
      "Epoch 37/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9299 - loss: 0.1614 - val_accuracy: 0.7995 - val_loss: 1.0565\n",
      "Epoch 38/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9215 - loss: 0.1642 - val_accuracy: 0.7737 - val_loss: 0.9795\n",
      "Epoch 39/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9241 - loss: 0.1671 - val_accuracy: 0.8032 - val_loss: 1.2371\n",
      "Epoch 40/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9223 - loss: 0.1645 - val_accuracy: 0.7935 - val_loss: 1.1365\n",
      "Epoch 41/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9216 - loss: 0.1613 - val_accuracy: 0.8065 - val_loss: 1.1028\n",
      "Epoch 42/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9299 - loss: 0.1518 - val_accuracy: 0.7978 - val_loss: 1.1073\n",
      "Epoch 43/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9230 - loss: 0.1585 - val_accuracy: 0.7984 - val_loss: 1.1081\n",
      "Epoch 44/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9262 - loss: 0.1587 - val_accuracy: 0.7984 - val_loss: 1.2228\n",
      "Epoch 45/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9206 - loss: 0.1559 - val_accuracy: 0.7995 - val_loss: 1.2657\n",
      "Epoch 46/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9311 - loss: 0.1490 - val_accuracy: 0.8059 - val_loss: 1.1676\n",
      "Epoch 47/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9279 - loss: 0.1527 - val_accuracy: 0.8081 - val_loss: 1.2392\n",
      "Epoch 48/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9227 - loss: 0.1603 - val_accuracy: 0.8038 - val_loss: 1.2459\n",
      "Epoch 49/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9263 - loss: 0.1533 - val_accuracy: 0.7968 - val_loss: 1.2555\n",
      "Epoch 50/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9272 - loss: 0.1479 - val_accuracy: 0.7871 - val_loss: 1.2796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f91dc3f2d10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Do tuning of a NN for the LSTM features, also try the random forest with LSTM features (you can load the features from the csv file instead of running the LSTM model again)\n",
    "#BTW I remembered how to get the last hidden state of the LSTM with a newly compiled model. It turns out that you had to skip the weights of the last layer of the old model when setting the weights of the new model.\n",
    "#I have updated the code in the notebook to reflect this. It was important that we set return_sequences=False in the last LSTM layer of the new model, so that the output of the LSTM is the last hidden state and not the sequence of hidden states.\n",
    "\n",
    "features_df = pd.read_csv('features.csv')\n",
    "features = features_df.values\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Dense(256, activation='relu', input_shape=(features.shape[1],)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, beta['activity_class'], test_size=0.2)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.21      0.32       194\n",
      "           1       0.78      0.96      0.86      1342\n",
      "           2       0.76      0.34      0.47       324\n",
      "\n",
      "    accuracy                           0.78      1860\n",
      "   macro avg       0.76      0.50      0.55      1860\n",
      "weighted avg       0.77      0.78      0.74      1860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, beta['activity_class'], test_size=0.2)\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
