{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-08 16:12:51.288206: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-08 16:12:51.667801: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-08 16:12:51.798521: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-08 16:12:52.542828: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-08 16:12:55.509556: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, LSTM, TimeDistributed, BatchNormalization, Dropout, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import pandas as pd \n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLM():\n",
    "    def __init__(self, layers, n_chars, dropouts, trainables,lr):\n",
    "        self.layers = layers\n",
    "        self.n_chars = n_chars\n",
    "        self.dropouts = dropouts\n",
    "        self.trainables = trainables\n",
    "        self.lr = lr\n",
    "        self.model = None\n",
    "        self.build_model()\n",
    "    def build_model(self):\n",
    "        # create a squential model and assigned to self.model\n",
    "        self.model = Sequential()\n",
    "        # Add a BatchNormalization layer input layer, input shape should be None, number of characters\n",
    "        self.model.add(BatchNormalization(input_shape = (None,self.n_chars)))\n",
    "        \n",
    "        # Add n LSTM layers, where n is defined by the variable layers. This will be a list of ints where each int is the number of units in the LSTM layer. i.e [256, 512] will add two layers with 256 and 512 neurons respectively\n",
    "        # This loop should also include the dropout values contained in the dropouts variable (list of floats). \n",
    "        # And it should also include the trainable parameter from the trainables variable (list of booleans). \n",
    "        print(self.layers)\n",
    "        for i in range(len(self.layers)):\n",
    "            if i ==len(self.layers)-1:\n",
    "                print(self.layers[i], \"False\")\n",
    "                self.model.add(LSTM(units=self.layers[i], trainable=self.trainables[i], return_sequences=False, dropout=self.dropouts[i]))\n",
    "            else:\n",
    "                print(self.layers[i], \"True\")\n",
    "                self.model.add(LSTM(units=self.layers[i], trainable=self.trainables[i], return_sequences=True, dropout=self.dropouts[i]))\n",
    "            \n",
    "        # add another BatchNormalization layer, this time no need to specify input shape\n",
    "        self.model.add(BatchNormalization())\n",
    "        # Finally add an output layer, it should be a Dense layer with n_chars units and softmax activation. Also, this layer should be containd in a TimeDistributed layer, so that it can be applied to each character in the sequence.\n",
    "\n",
    "        # compile the model with Adam optimizer and categorical_crossentropy loss. Don't forget to set the learning rate to the value contained in the lr variable with optimizer = Adam(learning_rate=self.lr)\n",
    "        optimizer = Adam(learning_rate=self.lr)\n",
    "        self.model.compile(optimizer=optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "#Fixed parameters\n",
    "PROCESSING_FIXED = {'start_char': 'G', \n",
    "                    'end_char': 'E', \n",
    "                    'pad_char': 'A'}\n",
    "\n",
    "INDICES_TOKEN = {0: 'c',\n",
    "                 1: 'C',\n",
    "                 2: '(',\n",
    "                 3: ')',\n",
    "                 4: 'O',\n",
    "                 5: '1',\n",
    "                 6: '2',\n",
    "                 7: '=',\n",
    "                 8: 'N',\n",
    "                 9: '@',\n",
    "                 10: '[',\n",
    "                 11: ']',\n",
    "                 12: 'n',\n",
    "                 13: '3',\n",
    "                 14: 'H',\n",
    "                 15: 'F',\n",
    "                 16: '4',\n",
    "                 17: '-',\n",
    "                 18: 'S',\n",
    "                 19: 'Cl',\n",
    "                 20: '/',\n",
    "                 21: 's',\n",
    "                 22: 'o',\n",
    "                 23: '5',\n",
    "                 24: '+',\n",
    "                 25: '#',\n",
    "                 26: '\\\\',\n",
    "                 27: 'Br',\n",
    "                 28: 'P',\n",
    "                 29: '6',\n",
    "                 30: 'I',\n",
    "                 31: '7',\n",
    "                 32: PROCESSING_FIXED['start_char'],\n",
    "                 33: PROCESSING_FIXED['end_char'],\n",
    "                 34: PROCESSING_FIXED['pad_char']}                \n",
    "TOKEN_INDICES = {v: k for k, v in INDICES_TOKEN.items()}\n",
    "\n",
    "def smi_tokenizer(smi):\n",
    "    \"\"\"\n",
    "    Tokenize a SMILES\n",
    "    \"\"\"\n",
    "    pattern =  \"(\\[|\\]|Xe|Ba|Rb|Ra|Sr|Dy|Li|Kr|Bi|Mn|He|Am|Pu|Cm|Pm|Ne|Th|Ni|Pr|Fe|Lu|Pa|Fm|Tm|Tb|Er|Be|Al|Gd|Eu|te|As|Pt|Lr|Sm|Ca|La|Ti|Te|Ac|Si|Cf|Rf|Na|Cu|Au|Nd|Ag|Se|se|Zn|Mg|Br|Cl|U|V|K|C|B|H|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%\\d{2}|\\d)\"\n",
    "    regex = re.compile(pattern)\n",
    "    tokens = [token for token in regex.findall(smi)]\n",
    "\n",
    "    return tokens        \n",
    "\n",
    "class onehotencoder():\n",
    "    def __init__(self, max_chars=90):\n",
    "        # here we define the class variables that will be used in the functions. If we want to use them in the class, we need to use self. in front of the variable name\n",
    "        # then when we want to use the class variable in the function, we pass self as the first argument to the function and we can access the class variables\n",
    "        self.max_chars = max_chars\n",
    "    def smiles_to_num(self, smiles):\n",
    "        tokens = smi_tokenizer(smiles)\n",
    "        tokens = [PROCESSING_FIXED['start_char']] + tokens +[PROCESSING_FIXED['end_char']]\n",
    "        tokens+= [PROCESSING_FIXED['pad_char']]* (self.max_chars-len(tokens))\n",
    "        num_tokens = [TOKEN_INDICES[t] for t in tokens]\n",
    "        #print(tokens)\n",
    "        #create numbers\n",
    "        return np.asarray(tokens) #Return the numbers as a numpy array\n",
    "    def num_to_onehot(self,tokens):\n",
    "        onehot = np.zeros((len(tokens), len(INDICES_TOKEN)))\n",
    "        for i,token in enumerate(tokens):\n",
    "            onehot[i, TOKEN_INDICES[token]] = 1\n",
    "        return onehot\n",
    "        #This function will convert the numbers to one hot encoding. It should take a list of numbers and return a list of one hot encoded vectors with the shape (max_chars, 35)\n",
    "    def generate_data(self, smiles):\n",
    "        nums = self.smiles_to_num(smiles)\n",
    "        data = self.num_to_onehot(nums)\n",
    "        return np.asarray(data)\n",
    "        #This function should take a list of smiles strings and return a numpy array of one hot encoded vectors with the shape (number of smiles, max_chars, 35)\n",
    "        #You can use the smiles_to_num and num_to_onehot functions to help you with this\n",
    "        #This function will be used to generate the training and validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "chem_model = load_model(f'../pretraining/LSTM/{48:03d}.keras') #load the model from file\n",
    "print('model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27698/3666820747.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  beta[\"activity_class\"] = beta[\"activity_class\"].replace(\"very_active\", 2)\n"
     ]
    }
   ],
   "source": [
    "beta = pd.read_csv('beta_activity_class.csv') #Clean CSV file with beta secretase smiles and activity\n",
    "beta[\"activity_class\"].value_counts()\n",
    "#dropna of activity_class\n",
    "beta = beta.dropna(subset=[\"activity_class\"])\n",
    "#transfor activity_class to 0,1,2\n",
    "beta[\"activity_class\"] = beta[\"activity_class\"].replace(\"moderately_active\", 1)\n",
    "beta[\"activity_class\"] = beta[\"activity_class\"].replace(\"inactive\", 0)\n",
    "beta[\"activity_class\"] = beta[\"activity_class\"].replace(\"very_active\", 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ligand SMILES</th>\n",
       "      <th>activity_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCSc1cnc(cn1)C(=O)Nc1cccc(c1)[C@]1(C)CCSC(N)=N1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C[C@]1(CCSC(N)=N1)c1cc(NC(=O)CCc2ccc(O)c(O)c2)...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COc1nc(nc(C)c1F)N1C[C@H]2C(=O)N(C)C(=N)N[C@]2(...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COc1nc(nc(C)c1F)N1C[C@H]2C(=O)N(C)C(=N)N[C@]2(...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COc1nc(nc(C)c1F)N1C[C@H]2C(=O)N(C)C(=N)N[C@]2(...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15992</th>\n",
       "      <td>FC(F)(F)c1ccc(N\\N=C\\c2coc3cc4oc(cc4cc3c2=O)-c2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15993</th>\n",
       "      <td>COc1ccc(cc1)-c1cc2cc3c(cc2o1)occ(\\C=N\\Nc1ccc(c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15994</th>\n",
       "      <td>COc1ccc(CN(CCN(C)CCN)c2ccccn2)cc1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>Fc1cccc(c1)-c1cc2c(ccc3c2occ(\\C=N\\Nc2ccc(cc2)C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>CCC(=O)CNC(=O)Nc1nc2ccc(cc2s1)-c1oc2c(OC)c(O)c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13155 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Ligand SMILES  activity_class\n",
       "0        CCSc1cnc(cn1)C(=O)Nc1cccc(c1)[C@]1(C)CCSC(N)=N1               1\n",
       "1      C[C@]1(CCSC(N)=N1)c1cc(NC(=O)CCc2ccc(O)c(O)c2)...               1\n",
       "2      COc1nc(nc(C)c1F)N1C[C@H]2C(=O)N(C)C(=N)N[C@]2(...               1\n",
       "3      COc1nc(nc(C)c1F)N1C[C@H]2C(=O)N(C)C(=N)N[C@]2(...               1\n",
       "4      COc1nc(nc(C)c1F)N1C[C@H]2C(=O)N(C)C(=N)N[C@]2(...               1\n",
       "...                                                  ...             ...\n",
       "15992  FC(F)(F)c1ccc(N\\N=C\\c2coc3cc4oc(cc4cc3c2=O)-c2...               0\n",
       "15993  COc1ccc(cc1)-c1cc2cc3c(cc2o1)occ(\\C=N\\Nc1ccc(c...               0\n",
       "15994                  COc1ccc(CN(CCN(C)CCN)c2ccccn2)cc1               0\n",
       "15995  Fc1cccc(c1)-c1cc2c(ccc3c2occ(\\C=N\\Nc2ccc(cc2)C...               0\n",
       "15996  CCC(=O)CNC(=O)Nc1nc2ccc(cc2s1)-c1oc2c(OC)c(O)c...               1\n",
       "\n",
       "[13155 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also, remove any smiles string that contains a character NOT in our vocabulary (excluding pad, start and end chars). Hint: allowed_chars = [t for t in TOKEN_INDICES.keys()][:-3]\n",
    "allowed_chars = [t for t in TOKEN_INDICES.keys()][:-3]\n",
    "beta = beta[beta['Ligand SMILES'].apply(lambda x: all(char in allowed_chars for char in x))]\n",
    "#drop data longer than 90 characters\n",
    "beta = beta[beta['Ligand SMILES'].apply(lambda x: len(x)<=90)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ligand SMILES</th>\n",
       "      <th>activity_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCSc1cnc(cn1)C(=O)Nc1cccc(c1)[C@]1(C)CCSC(N)=N1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C[C@]1(CCSC(N)=N1)c1cc(NC(=O)CCc2ccc(O)c(O)c2)...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COc1nc(nc(C)c1F)N1C[C@H]2C(=O)N(C)C(=N)N[C@]2(...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nc1ccc2ccccc2n1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nc1cccc(CCc2ccccc2)n1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15991</th>\n",
       "      <td>Fc1cccc(c1)-c1cc2cc3c(cc2o1)occ(\\C=N\\Nc1ccc(cc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15993</th>\n",
       "      <td>COc1ccc(cc1)-c1cc2cc3c(cc2o1)occ(\\C=N\\Nc1ccc(c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15994</th>\n",
       "      <td>COc1ccc(CN(CCN(C)CCN)c2ccccn2)cc1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>Fc1cccc(c1)-c1cc2c(ccc3c2occ(\\C=N\\Nc2ccc(cc2)C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>CCC(=O)CNC(=O)Nc1nc2ccc(cc2s1)-c1oc2c(OC)c(O)c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9300 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Ligand SMILES  activity_class\n",
       "0        CCSc1cnc(cn1)C(=O)Nc1cccc(c1)[C@]1(C)CCSC(N)=N1               1\n",
       "1      C[C@]1(CCSC(N)=N1)c1cc(NC(=O)CCc2ccc(O)c(O)c2)...               1\n",
       "2      COc1nc(nc(C)c1F)N1C[C@H]2C(=O)N(C)C(=N)N[C@]2(...               1\n",
       "5                                        Nc1ccc2ccccc2n1               0\n",
       "6                                  Nc1cccc(CCc2ccccc2)n1               0\n",
       "...                                                  ...             ...\n",
       "15991  Fc1cccc(c1)-c1cc2cc3c(cc2o1)occ(\\C=N\\Nc1ccc(cc...               0\n",
       "15993  COc1ccc(cc1)-c1cc2cc3c(cc2o1)occ(\\C=N\\Nc1ccc(c...               0\n",
       "15994                  COc1ccc(CN(CCN(C)CCN)c2ccccn2)cc1               0\n",
       "15995  Fc1cccc(c1)-c1cc2c(ccc3c2occ(\\C=N\\Nc2ccc(cc2)C...               0\n",
       "15996  CCC(=O)CNC(=O)Nc1nc2ccc(cc2s1)-c1oc2c(OC)c(O)c...               1\n",
       "\n",
       "[9300 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list with the one hot vectors of the smiles\n",
    "encoder = onehotencoder(max_chars=92)\n",
    "smiles_arr = [encoder.generate_data(s) for s in beta['Ligand SMILES']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_x = np.zeros((9300,92,35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9300):\n",
    "    inputs_x[i] = smiles_arr[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scro4068/.local/lib/python3.10/site-packages/keras/src/layers/normalization/batch_normalization.py:143: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1024, 256]\n",
      "1024 True\n",
      "256 False\n",
      "model created\n"
     ]
    }
   ],
   "source": [
    "#compile the new model\n",
    "layers = [1024,256]\n",
    "dropouts = [0.40, 0.40]\n",
    "trainables = [True, True]\n",
    "lr = 0.001\n",
    "epochs = 80\n",
    "new_model = CLM(\n",
    "    layers=layers,\n",
    "    dropouts=dropouts,\n",
    "    trainables=trainables,\n",
    "    lr=lr,\n",
    "    n_chars=35\n",
    "    )\n",
    "print('model created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the weights of the new model to the weights of the old model except the last layer\n",
    "new_model.model.set_weights(chem_model.get_weights()[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9300, 92, 35)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict from smiles arr and get the last hidden state of the LSTM\n",
    "hidden = chem_model.predict(inputs_x)\n",
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = hidden.reshape(9300, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.19550545e-02, 7.91305244e-01, 1.34980581e-07, ...,\n",
       "       2.86512675e-10, 5.14766683e-08, 9.99995828e-01], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save the features to a csv file\n",
    "features_df = pd.DataFrame(features)\n",
    "features_df.to_csv('features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do tuning of a NN for the LSTM features, also try the random forest with LSTM features (you can load the features from the csv file instead of running the LSTM model again)\n",
    "#BTW I remembered how to get the last hidden state of the LSTM with a newly compiled model. It turns out that you had to skip the weights of the last layer of the old model when setting the weights of the new model.\n",
    "#I have updated the code in the notebook to reflect this. It was important that we set return_sequences=False in the last LSTM layer of the new model, so that the output of the LSTM is the last hidden state and not the sequence of hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
