{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c58fa8f-f719-4cf5-beab-bde11ee407db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-23 08:37:38.970787: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-23 08:37:38.986035: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-23 08:37:38.990578: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-23 08:37:39.002222: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-23 08:37:41.066114: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/biggin/b196/scro4068/mambaforge/envs/keras_tf2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import keras_nlp\n",
    "import tensorflow\n",
    "import keras_tuner\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1def3dca-e1cc-4562-8993-cb78c2f0b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixed parameters\n",
    "PROCESSING_FIXED = {'start_char': \"<s>\",\n",
    "                    'end_char': \"</s>\", \n",
    "                    'pad_char': \"<pad>\",\n",
    "                    'mas_char': \"<mask>\"}\n",
    "\n",
    "INDICES_TOKEN = {0: 'c',\n",
    "                 1: 'C',\n",
    "                 2: '(',\n",
    "                 3: ')',\n",
    "                 4: 'O',\n",
    "                 5: '1',\n",
    "                 6: '2',\n",
    "                 7: '=',\n",
    "                 8: 'N',\n",
    "                 9: '@',\n",
    "                 10: '[',\n",
    "                 11: ']',\n",
    "                 12: 'n',\n",
    "                 13: '3',\n",
    "                 14: 'H',\n",
    "                 15: 'F',\n",
    "                 16: '4',\n",
    "                 17: '-',\n",
    "                 18: 'S',\n",
    "                 19: 'Cl',\n",
    "                 20: '/',\n",
    "                 21: 's',\n",
    "                 22: 'o',\n",
    "                 23: '5',\n",
    "                 24: '+',\n",
    "                 25: '#',\n",
    "                 26: '\\\\',\n",
    "                 27: 'Br',\n",
    "                 28: 'P',\n",
    "                 29: '6',\n",
    "                 30: 'I',\n",
    "                 31: '7',\n",
    "                 32: PROCESSING_FIXED['start_char'],\n",
    "                 33: PROCESSING_FIXED['end_char'],\n",
    "                 34: PROCESSING_FIXED['pad_char'],\n",
    "                 35: PROCESSING_FIXED['mas_char']}                \n",
    "TOKEN_INDICES = {v: k for k, v in INDICES_TOKEN.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cbd9350-d859-4509-ac6d-a83cdfe62e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-23 08:37:46.532151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6187 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:65:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "tokenizer = keras_nlp.models.RobertaTokenizer(vocabulary=TOKEN_INDICES, merges=['C l', 'B r'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af0a4c02-19a9-41c8-bdc1-254c2a8814ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessor = keras_nlp.models.RobertaMaskedLMPreprocessor(\n",
    "    tokenizer,\n",
    "    sequence_length=90,\n",
    "    mask_selection_rate=0.15,\n",
    "    mask_selection_length=16,\n",
    "    mask_token_rate=0.8,\n",
    "    random_token_rate=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cd18369-fee4-4e94-b725-5de1acc98a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaHyperModel(keras_tuner.HyperModel):\n",
    "    def __init__(self, vocabulary_size, max_sequence_length, preprocessor):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def build(self, hp):\n",
    "        hidden_dim = hp.Int('hidden_dim', min_value=128, max_value=512, step=32)\n",
    "        num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=1)\n",
    "        dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.3, step=0.05)\n",
    "        intermediate_dim = hp.Int('intermediate_dim', min_value=256, max_value=1024, step=64)\n",
    "        num_layers = hp.Int('num_layers', min_value=2, max_value=6, step=1)\n",
    "        learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=5e-5, sampling='log')\n",
    "\n",
    "        backbone = keras_nlp.models.RobertaBackbone(\n",
    "            vocabulary_size=self.vocabulary_size,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            hidden_dim=hidden_dim,\n",
    "            intermediate_dim=intermediate_dim,\n",
    "            max_sequence_length=self.max_sequence_length,\n",
    "            dropout=dropout_rate,\n",
    "        )\n",
    "        model = keras_nlp.models.RobertaMaskedLM(\n",
    "            backbone=backbone,\n",
    "            preprocessor=self.preprocessor,\n",
    "        )\n",
    "        # dropouts?\n",
    "        #model.add(layers.Dropout(rate=dropout_rate))\n",
    "        model.compile(\n",
    "            optimizer=tensorflow.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa88d7e1-7b4e-4e15-a3ee-3de761f9b921",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.RandomSearch(\n",
    "    hypermodel=RobertaHyperModel(\n",
    "        vocabulary_size=36,\n",
    "        max_sequence_length=90,\n",
    "        preprocessor=preprocessor,\n",
    "    ),\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    overwrite=True,\n",
    "    directory=\"hyper_tuning\",\n",
    "    project_name=\"roberta_smiles\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "665689ce-2d45-4812-99db-654d07aff755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 6\n",
      "hidden_dim (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 128, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "num_heads (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 8, 'step': 1, 'sampling': 'linear'}\n",
      "dropout_rate (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.3, 'step': 0.05, 'sampling': 'linear'}\n",
      "intermediate_dim (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 256, 'max_value': 1024, 'step': 64, 'sampling': 'linear'}\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 6, 'step': 1, 'sampling': 'linear'}\n",
      "learning_rate (Float)\n",
      "{'default': 1e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 5e-05, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "182a5fa9-a2be-4d50-8afb-8b13e6e1d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data from data/us_pharma_patent_data_lowe_smiles_can_unique_stereochem.txt\n",
    "data = pd.read_csv('data/us_pharma_patent_data_lowe_smiles_can_unique_stereochem.txt', sep='\\t', header=None)\n",
    "#Remember to drop missing values and duplicates\n",
    "data = data.dropna().drop_duplicates()\n",
    "#Also, remove any smiles string that contains a character NOT in our vocabulary (excluding pad, start and end chars). Hint: allowed_chars = [t for t in TOKEN_INDICES.keys()][:-3]\n",
    "allowed_chars = [t for t in TOKEN_INDICES.keys()][:-3]\n",
    "data = data[data[0].apply(lambda x: all(char in allowed_chars for char in x))]\n",
    "#drop data longer than 90 characters\n",
    "data = data[data[0].apply(lambda x: len(x)<=90)]\n",
    "#Split the data into train and test sets with a 80/20 split. Don't forget to reset the index of the dataframes before splitting, so then we can use the train.index and test.index to create the generators\n",
    "data = data.reset_index(drop=True)\n",
    "#train_data, test_data = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5a89b0d-0bea-4935-9239-e1c0b118104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_idx = np.loadtxt('data/train_data_idx.txt')\n",
    "test_idx = np.loadtxt('data/test_data_idx.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27467610-4832-4956-b745-8d518bae950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[0].iloc[train_idx]\n",
    "test_data = data[0].iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b60eb9b-5c1d-4d90-bb04-69964f2281b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 18 Complete [02h 03m 03s]\n",
      "val_loss: 0.717340350151062\n",
      "\n",
      "Best val_loss So Far: 0.45886924862861633\n",
      "Total elapsed time: 21h 56m 20s\n",
      "\n",
      "Search: Running Trial #19\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "288               |512               |hidden_dim\n",
      "3                 |6                 |num_heads\n",
      "0.15              |0.2               |dropout_rate\n",
      "832               |256               |intermediate_dim\n",
      "3                 |4                 |num_layers\n",
      "1.445e-05         |2.3983e-05        |learning_rate\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m 9758/15288\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 17ms/step - accuracy: 0.1198 - loss: 1.4357 - sparse_categorical_accuracy: 0.2660"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15288/15288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 20ms/step - accuracy: 0.3684 - loss: 0.8711 - sparse_categorical_accuracy: 0.4341 - val_accuracy: 0.4373 - val_loss: 0.8347 - val_sparse_categorical_accuracy: 0.4484\n",
      "Epoch 4/10\n",
      "\u001b[1m 3698/15288\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:37\u001b[0m 19ms/step - accuracy: 0.3700 - loss: 0.8346 - sparse_categorical_accuracy: 0.4475"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15288/15288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 20ms/step - accuracy: 0.4908 - loss: 0.7890 - sparse_categorical_accuracy: 0.4806 - val_accuracy: 0.4878 - val_loss: 0.7412 - val_sparse_categorical_accuracy: 0.5134\n",
      "Epoch 7/10\n",
      "\u001b[1m 3420/15288\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:41\u001b[0m 19ms/step - accuracy: 0.4802 - loss: 0.7581 - sparse_categorical_accuracy: 0.5016"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15288/15288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 20ms/step - accuracy: 0.5004 - loss: 0.7093 - sparse_categorical_accuracy: 0.5273 - val_accuracy: 0.5114 - val_loss: 0.6401 - val_sparse_categorical_accuracy: 0.5581\n",
      "Epoch 10/10\n",
      "\u001b[1m 3606/15288\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:39\u001b[0m 19ms/step - accuracy: 0.4670 - loss: 0.6828 - sparse_categorical_accuracy: 0.5482"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15288/15288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 18ms/step - accuracy: 0.1568 - loss: 0.9242 - sparse_categorical_accuracy: 0.3108 - val_accuracy: 0.4349 - val_loss: 0.8502 - val_sparse_categorical_accuracy: 0.3384\n",
      "Epoch 3/10\n",
      "\u001b[1m 6086/15288\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:25\u001b[0m 16ms/step - accuracy: 0.4407 - loss: 0.8610 - sparse_categorical_accuracy: 0.3449"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12841/15288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m38s\u001b[0m 16ms/step - accuracy: 0.5770 - loss: 0.7531 - sparse_categorical_accuracy: 0.5004"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15288/15288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 17ms/step - accuracy: 0.5690 - loss: 0.6571 - sparse_categorical_accuracy: 0.5675 - val_accuracy: 0.5812 - val_loss: 0.5493 - val_sparse_categorical_accuracy: 0.6379\n",
      "Epoch 10/10\n",
      "\u001b[1m 1740/15288\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:34\u001b[0m 16ms/step - accuracy: 0.5604 - loss: 0.6233 - sparse_categorical_accuracy: 0.5955"
     ]
    }
   ],
   "source": [
    "tuner.search(x=train_data, validation_data=test_data, epochs=10, callbacks=[keras.callbacks.TensorBoard(\"tensorboard/tb_logs\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bdea35-ddbb-49b6-b0a1-9e30c628195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8bc76a-9fd2-41d0-b8f6-14a6e088d2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model[0].summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01820184-6f1b-4412-b0b3-d1c1ccaebaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model[0].get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a02ad4-1cbb-47f5-80d5-7fe4abbf8f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model[0].save('best_roberta.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f34e02-06f3-44a1-8458-49719e4cb25a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
