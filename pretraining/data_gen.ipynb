{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:16:16.811496: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-02 15:16:17.038252: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-02 15:16:17.086765: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-02 15:16:17.408820: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-02 15:16:19.753505: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSING_FIXED = {'start_char': 'G', \n",
    "                    'end_char': 'E', \n",
    "                    'pad_char': 'A'}\n",
    "\n",
    "INDICES_TOKEN = {0: 'c',\n",
    "                 1: 'C',\n",
    "                 2: '(',\n",
    "                 3: ')',\n",
    "                 4: 'O',\n",
    "                 5: '1',\n",
    "                 6: '2',\n",
    "                 7: '=',\n",
    "                 8: 'N',\n",
    "                 9: '@',\n",
    "                 10: '[',\n",
    "                 11: ']',\n",
    "                 12: 'n',\n",
    "                 13: '3',\n",
    "                 14: 'H',\n",
    "                 15: 'F',\n",
    "                 16: '4',\n",
    "                 17: '-',\n",
    "                 18: 'S',\n",
    "                 19: 'Cl',\n",
    "                 20: '/',\n",
    "                 21: 's',\n",
    "                 22: 'o',\n",
    "                 23: '5',\n",
    "                 24: '+',\n",
    "                 25: '#',\n",
    "                 26: '\\\\',\n",
    "                 27: 'Br',\n",
    "                 28: 'P',\n",
    "                 29: '6',\n",
    "                 30: 'I',\n",
    "                 31: '7',\n",
    "                 32: PROCESSING_FIXED['start_char'],\n",
    "                 33: PROCESSING_FIXED['end_char'],\n",
    "                 34: PROCESSING_FIXED['pad_char']}                \n",
    "TOKEN_INDICES = {v: k for k, v in INDICES_TOKEN.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smi_tokenizer(smi):\n",
    "    \"\"\"\n",
    "    Tokenize a SMILES\n",
    "    \"\"\"\n",
    "    pattern =  \"(\\[|\\]|Xe|Ba|Rb|Ra|Sr|Dy|Li|Kr|Bi|Mn|He|Am|Pu|Cm|Pm|Ne|Th|Ni|Pr|Fe|Lu|Pa|Fm|Tm|Tb|Er|Be|Al|Gd|Eu|te|As|Pt|Lr|Sm|Ca|La|Ti|Te|Ac|Si|Cf|Rf|Na|Cu|Au|Nd|Ag|Se|se|Zn|Mg|Br|Cl|U|V|K|C|B|H|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%\\d{2}|\\d)\"\n",
    "    regex = re.compile(pattern)\n",
    "    tokens = [token for token in regex.findall(smi)]\n",
    "\n",
    "    return tokens        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class onehotencoder():\n",
    "    def __init__(self, max_chars=90):\n",
    "        # here we define the class variables that will be used in the functions. If we want to use them in the class, we need to use self. in front of the variable name\n",
    "        # then when we want to use the class variable in the function, we pass self as the first argument to the function and we can access the class variables\n",
    "        self.max_chars = max_chars\n",
    "    def smiles_to_num(self, smiles):\n",
    "        tokens = smi_tokenizer(smiles)\n",
    "        tokens = [PROCESSING_FIXED['start_char']] + tokens +[PROCESSING_FIXED['end_char']]\n",
    "        tokens+= [PROCESSING_FIXED['pad_char']]* (self.max_chars-len(tokens))\n",
    "        num_tokens = [TOKEN_INDICES[t] for t in tokens]\n",
    "        #print(tokens)\n",
    "        #create numbers\n",
    "        return np.asarray(num_tokens) #Return the numbers as a numpy array\n",
    "    def num_to_onehot(self,tokens):\n",
    "        onehot = np.zeros((len(tokens), len(INDICES_TOKEN)))\n",
    "        for i,token in enumerate(tokens):\n",
    "            onehot[i, TOKEN_INDICES[token]] = 1\n",
    "        return onehot\n",
    "        #This function will convert the numbers to one hot encoding. It should take a list of numbers and return a list of one hot encoded vectors with the shape (max_chars, 35)\n",
    "    def generate_data(self, smiles):\n",
    "        nums = self.smiles_to_num(smiles)\n",
    "        data = self.num_to_onehot(nums)\n",
    "        return np.asarray(data)\n",
    "        #This function should take a list of smiles strings and return a numpy array of one hot encoded vectors with the shape (number of smiles, max_chars, 35)\n",
    "        #You can use the smiles_to_num and num_to_onehot functions to help you with this\n",
    "        #This function will be used to generate the training and validation data\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, list_IDs, batch_size, data, max_len, num_chars, use_multiprocessing=True, workers=6, shuffle=True):\n",
    "        super().__init__(use_multiprocessing=use_multiprocessing, workers=workers)\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.data = data\n",
    "        self.num_chars = num_chars\n",
    "        self.max_len = max_len\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = np.load('data/' + ID + '.npy')\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
    "    #You can give this a try with the tutorial and see if you can get it to work https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "    #This will be used to generate the data for the model. It should take a list of smiles strings and the corresponding labels and return the one hot encoded vectors in batches\n",
    "    #You can use the onehotencoder class to help you with this.\n",
    "    #It is important to follow the function signature of the keras.utils.Sequence class so that it can be used with the model.fit_generator function\n",
    "    #This is al detailed in the tutorial above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_smiles = \"CCccOO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodere = onehotencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = encodere.generate_data([test_smiles])\n",
    "print(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
